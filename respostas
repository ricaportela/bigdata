Qual o objetivo​ ​ do​ ​ comando​ ​ cache​ ​ em​ ​ Spark?
R.: o cache normalmente é melhorar a performance do código, reutilizando os resultados nas iterações

O​ ​ mesmo​ ​ código​ ​ implementado​ ​ em​ ​ Spark​ ​ é ​ ​ normalmente​ ​ mais​ ​ rápido​ ​ que​ ​ a ​ ​ implementação​ ​ equivalente​ ​ em
MapReduce.​ ​ Por​ ​ quê?
R.: A JVM do Spark fica em execução constante em cada nó, iniciando um Thread. Enquanto o mapreduce inicia uma nova JVM a cada execução, e o resuldo do Job é escrito em disco. Portanto é mais lento que o Spark.


Qual​ ​ é ​ ​ a ​ ​ função​ ​ do​ ​ SparkContext​ ?
R.: O SparkContext faz a conexão com o cluster, pode ser utilizado para criar RDD também.



Explique​ ​ com​ ​ suas​ ​ palavras​ ​ ​ o ​ ​ que​ ​ é ​ ​ Resilient​ ​ Distributed​ ​ Datasets​​ ​ (RDD).
R.: São arquivos resistentes a falhas (Resilent)


GroupByKey​ ​ é ​ ​ menos​ ​ eficiente​ ​ que​ ​ reduceByKey​ ​ em​ ​ grandes​ ​ dataset.​ ​ Por​ ​ quê?
R.: O GroupbyKey força a gravação de dados em disco e um grande uso de memória.
